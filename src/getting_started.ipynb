{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert your trained models to ONNX format\n",
    "Dlacc is able to accelerate trained deep learning models. First, you must convert your trained models (pytorch, tensorflow, mxnet, etc) to onnx format. Open Neural Network Exchange (ONNX) is an open standard format for representing machine learning models. ONNX is supported by a community of partners who have implemented it in many frameworks and tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch to ONNX\n",
    "We import pre-trained resnet18 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mac_yuan/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to construct an example input. contruct_dummy_input() is a useful function for this. You just need to specify a (list of) valid input(s) and also their corresponding datatype(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input.1 : Float(10, 3, 224, 224, strides=[150528, 50176, 224, 1], requires_grad=0, device=cpu),\n",
      "      %fc.weight : Float(1000, 512, strides=[512, 1], requires_grad=1, device=cpu),\n",
      "      %fc.bias : Float(1000, strides=[1], requires_grad=1, device=cpu),\n",
      "      %193 : Float(64, 3, 7, 7, strides=[147, 49, 7, 1], requires_grad=0, device=cpu),\n",
      "      %194 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %196 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %197 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %199 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %200 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %202 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %203 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %205 : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %206 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %208 : Float(128, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %209 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %211 : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %212 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %214 : Float(128, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %215 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %217 : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %218 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %220 : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %221 : Float(128, strides=[1], requires_grad=0, device=cpu),\n",
      "      %223 : Float(256, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %224 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %226 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %227 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %229 : Float(256, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %230 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %232 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %233 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %235 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %236 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %238 : Float(512, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %239 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %241 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %242 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %244 : Float(512, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %245 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %247 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %248 : Float(512, strides=[1], requires_grad=0, device=cpu),\n",
      "      %250 : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %251 : Float(512, strides=[1], requires_grad=0, device=cpu)):\n",
      "  %input.4 : Float(10, 64, 112, 112, strides=[802816, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[7, 7], pads=[3, 3, 3, 3], strides=[2, 2]](%input.1, %193, %194) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:443:0\n",
      "  %125 : Float(10, 64, 112, 112, strides=[802816, 12544, 112, 1], requires_grad=1, device=cpu) = onnx::Relu(%input.4) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/functional.py:1440:0\n",
      "  %input.8 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::MaxPool[kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%125) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/functional.py:797:0\n",
      "  %input.16 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.8, %196, %197) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:443:0\n",
      "  %129 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu(%input.16) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/functional.py:1440:0\n",
      "  %198 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%129, %199, %200) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:443:0\n",
      "  %132 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Add(%198, %input.8) # /home/mac_yuan/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py:80:0\n",
      "  %input.24 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu(%132) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/functional.py:1440:0\n",
      "  %input.32 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.24, %202, %203) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:443:0\n",
      "  %136 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu(%input.32) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/functional.py:1440:0\n",
      "  %204 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%136, %205, %206) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:443:0\n",
      "  %139 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Add(%204, %input.24) # /home/mac_yuan/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py:80:0\n",
      "  %input.40 : Float(10, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=1, device=cpu) = onnx::Relu(%139) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/functional.py:1440:0\n",
      "  %input.48 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%input.40, %208, %209) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:443:0\n",
      "  %143 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu(%input.48) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/functional.py:1440:0\n",
      "  %210 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%143, %211, %212) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:443:0\n",
      "  %213 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%input.40, %214, %215) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:443:0\n",
      "  %148 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Add(%210, %213) # /home/mac_yuan/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py:80:0\n",
      "  %input.60 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu(%148) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/functional.py:1440:0\n",
      "  %input.68 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.60, %217, %218) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:443:0\n",
      "  %152 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu(%input.68) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/functional.py:1440:0\n",
      "  %219 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%152, %220, %221) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:443:0\n",
      "  %155 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Add(%219, %input.60) # /home/mac_yuan/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py:80:0\n",
      "  %input.76 : Float(10, 128, 28, 28, strides=[100352, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu(%155) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/functional.py:1440:0\n",
      "  %input.84 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%input.76, %223, %224) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:443:0\n",
      "  %159 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu(%input.84) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/functional.py:1440:0\n",
      "  %225 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%159, %226, %227) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:443:0\n",
      "  %228 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%input.76, %229, %230) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:443:0\n",
      "  %164 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Add(%225, %228) # /home/mac_yuan/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py:80:0\n",
      "  %input.96 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu(%164) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/functional.py:1440:0\n",
      "  %input.104 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.96, %232, %233) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:443:0\n",
      "  %168 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu(%input.104) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/functional.py:1440:0\n",
      "  %234 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%168, %235, %236) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:443:0\n",
      "  %171 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Add(%234, %input.96) # /home/mac_yuan/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py:80:0\n",
      "  %input.112 : Float(10, 256, 14, 14, strides=[50176, 196, 14, 1], requires_grad=1, device=cpu) = onnx::Relu(%171) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/functional.py:1440:0\n",
      "  %input.120 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%input.112, %238, %239) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:443:0\n",
      "  %175 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Relu(%input.120) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/functional.py:1440:0\n",
      "  %240 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%175, %241, %242) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:443:0\n",
      "  %243 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[2, 2]](%input.112, %244, %245) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:443:0\n",
      "  %180 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Add(%240, %243) # /home/mac_yuan/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py:80:0\n",
      "  %input.132 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Relu(%180) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/functional.py:1440:0\n",
      "  %input.140 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%input.132, %247, %248) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:443:0\n",
      "  %184 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Relu(%input.140) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/functional.py:1440:0\n",
      "  %249 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%184, %250, %251) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:443:0\n",
      "  %187 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Add(%249, %input.132) # /home/mac_yuan/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py:80:0\n",
      "  %input.148 : Float(10, 512, 7, 7, strides=[25088, 49, 7, 1], requires_grad=1, device=cpu) = onnx::Relu(%187) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/functional.py:1440:0\n",
      "  %189 : Float(10, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%input.148) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/functional.py:1241:0\n",
      "  %190 : Float(10, 512, strides=[512, 1], requires_grad=1, device=cpu) = onnx::Flatten[axis=1](%189) # /home/mac_yuan/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/resnet.py:243:0\n",
      "  %191 : Float(10, 1000, strides=[1000, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%190, %fc.weight, %fc.bias) # /home/mac_yuan/.local/lib/python3.9/site-packages/torch/nn/modules/linear.py:103:0\n",
      "  return (%191)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def contruct_dummy_input(input_shape, input_dtype):\n",
    "    dummy_input = tuple(\n",
    "        [\n",
    "            torch.randn(*v).type(\n",
    "                {\n",
    "                    \"int32\": torch.int32,\n",
    "                    \"int64\": torch.int64,\n",
    "                    \"float32\": torch.float32,\n",
    "                    \"float64\": torch.float64,\n",
    "                }[input_dtype[i]]\n",
    "            )\n",
    "            for i, v in enumerate(input_shape)\n",
    "        ]\n",
    "    )\n",
    "    return dummy_input\n",
    "\n",
    "input_shape = [[10,3,224,224]]\n",
    "input_dtype = [\"float32\"]\n",
    "\n",
    "dummy_input = contruct_dummy_input(input_shape, input_dtype)\n",
    "model.eval()\n",
    "# Export the model\n",
    "torch.onnx.export(\n",
    "    model,  # model being run\n",
    "    dummy_input,  # model input (or a tuple for multiple inputs)\n",
    "    \"resnet18.onnx\",\n",
    "    export_params=True,  # store the trained parameter weights inside the model file\n",
    "    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter configuration\n",
    "Then we create a global json configuration file. The tool will run optimization process according to this json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"job_id\": \"100000\",\n",
    "    \"status\": 0,\n",
    "    \"model_name\" : \"resnet\",\n",
    "    \"model_path\": \"./resnet18.onnx\",\n",
    "    \"platform_type\": 0, \n",
    "    \"model_type\" : 2,\n",
    "    \"target\": \"llvm -mcpu cascadelake\",\n",
    "    \"model_config\":{\n",
    "        \"input_shape\":{\n",
    "            \"input.1\": [10,3,224,224],\n",
    "        },\n",
    "        \"input_dtype\":{\n",
    "            \"input.1\": \"float32\",\n",
    "        }\n",
    "    },\n",
    "    \"tuning_config\": {\n",
    "        \"mode\": \"ansor\",\n",
    "        \"num_measure_trials\": 24,\n",
    "        \"verbose_print\": 0\n",
    "    },\n",
    "    \"tuned_log\":\"\",\n",
    "    \"need_benchmark\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those fields are : \n",
    "- job_id : id of the job, random int\n",
    "- status:\n",
    "    - 0: ready\n",
    "    - 1: import to onnx finished\n",
    "    - 2: ansor tuning finished (time cost overhead, skippable if tunned_log specified)\n",
    "    - 3: compile finished\n",
    "    - 4: job done\n",
    "    - -1: error\n",
    "- model_name: name of the model\n",
    "- model_path: path to the model.\n",
    "    - an absolute local path if plateform_type==LOCAL\n",
    "    - a google storage bucket link if plateform_type==GOOGLESTORAGE\n",
    "- platform_type: type of source platform that stores the model file and input json.\n",
    "    \n",
    "    ```python\n",
    "    class PlateformType(enum.IntEnum):\n",
    "    \tLOCAL = 0\n",
    "    \tGOOGLESTORAGE = 1\n",
    "    \tAWSSTORAGE = 2\n",
    "    ```\n",
    "    \n",
    "- model_type: type of model. Only onnx format is supported by now.\n",
    "\n",
    "```python\n",
    "class ModelType(enum.IntEnum):\n",
    "    PT = 0\n",
    "    TF = 1\n",
    "    ONNX = 2\n",
    "    KERAS = 3\n",
    "```\n",
    "\n",
    "- target: target hardware backend information\n",
    "- model_config\n",
    "    - input_shape: shape of each input. **The first dimension must be batch size.**\n",
    "    - input_dtype: datatype of each input.\n",
    "- tuning_config: tuning parameter configuration\n",
    "    - mode: string value. ansor or autotvm. Only ansor for now.\n",
    "    - num_measure_trials: an int value. More trials, better performance, more time costs.\n",
    "        - when testing, 10 for a quick execution\n",
    "        - when in production, 20000 for best performance.\n",
    "    - verbose_print: if enbale verbose print\n",
    "- tuned_log: dev only. Tuning will not be executed if a tuned log is passed.\n",
    "- error_info: dev only. Exception information raised during execution.\n",
    "- need_benchmark: bool value. Whether need comparison with the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  ['input.1']\n",
      "Outputs:  ['191']\n"
     ]
    }
   ],
   "source": [
    "from optimum import Optimum\n",
    "import onnx\n",
    "model = onnx.load(\"./resnet18.onnx\")\n",
    "output =[node.name for node in model.graph.output]\n",
    "\n",
    "input_all = [node.name for node in model.graph.input]\n",
    "input_initializer =  [node.name for node in model.graph.initializer]\n",
    "net_feed_input = list(set(input_all)  - set(input_initializer))\n",
    "\n",
    "print('Inputs: ', net_feed_input)\n",
    "print('Outputs: ', output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization process may produce many error massages. This is normal because the optimization engine will try some invalid schedules. You can safely ignore them if the tuning can continue, because these errors are isolated from the main process. After optimization, optimized model, statistics, logs will be saved in ./outputs folder. The optimized model will be saved in ./ouputs/optimized_model folder, containing 3 files, deploy_graph.json, deploy_lib.tar, deploy_param.params. You can reutilize those 3 files later in your own production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31-05-2022-11:40:48][AnsorEngine] Run tuning for network=myresnet\n",
      "[31-05-2022-11:40:48][AnsorEngine] Extract tasks...\n",
      "[31-05-2022-11:40:55][AnsorEngine] Begin tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mac_yuan/.local/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "[11:41:00] ../src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:\n",
      "Placeholder: placeholder\n",
      "tensor.repl auto_unroll: 16\n",
      "parallel ax0@ax1@ (None)\n",
      "  for ax0 (None)\n",
      "    for ax1 (None)\n",
      "      for ax2 (None)\n",
      "        for ax3 (None)\n",
      "          for ax4 (None)\n",
      "            for rv0_rv1_fused_o (None)\n",
      "              vectorize rv0_rv1_fused_i (None)\n",
      "                tensor.rf = ...\n",
      "  for ax2 (None)\n",
      "    for ax0 (None)\n",
      "      for ax1 (None)\n",
      "        for ax2 (None)\n",
      "          for ax3 (None)\n",
      "            vectorize ax4 (None)\n",
      "              pad_temp = ...\n",
      "    for ax3 (None)\n",
      "      for ax4 (None)\n",
      "        for rv0_rv1_fused_i_v (None)\n",
      "          tensor.repl = ...\n",
      "\n",
      "with: [11:41:00] ../src/te/schedule/bound.cc:175: \n",
      "---------------------------------------------------------------\n",
      "An error occurred during the execution of TVM.\n",
      "For more information, please see: https://tvm.apache.org/docs/errors.html\n",
      "---------------------------------------------------------------\n",
      "  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(pad_temp, body=[tir.if_then_else(((((ax2 >= 1) && (ax2 < 113)) && (ax3 >= 1)) && (ax3 < 113)), placeholder[ax0, ax1, (ax2 - 1), (ax3 - 1), ax4], -3.40282e+38f)], axis=[iter_var(ax0, range(min=0, ext=10)), iter_var(ax1, range(min=0, ext=4)), iter_var(ax2, range(min=0, ext=114)), iter_var(ax3, range(min=0, ext=114)), iter_var(ax4, range(min=0, ext=16))], reduce_axis=[], tag=elemwise, attrs={}) along the loop nest specified by compute_at of consumer compute(tensor.rf, body=[reduce(combiner=comm_reducer(result=[max(x, y)], lhs=[x], rhs=[y], identity_element=[-3.40282e+38f]), source=[pad_temp[ax0, ax1, ((ax2*2) + floordiv(rv0.rv1.fused.inner, 3)), ((ax3*2) + floormod(rv0.rv1.fused.inner, 3)), ax4]], init=[], axis=[iter_var(rv0.rv1.fused.outer, range(min=0, ext=1))], where=(bool)1, value_index=0)], axis=[iter_var(ax0, range(min=0, ext=10)), iter_var(ax1, range(min=0, ext=4)), iter_var(ax2, range(min=0, ext=56)), iter_var(ax3, range(min=0, ext=56)), iter_var(ax4, range(min=0, ext=16)), iter_var(rv0.rv1.fused.inner, range(min=0, ext=9))], reduce_axis=[iter_var(rv0.rv1.fused.outer, range(min=0, ext=1))], tag=, attrs={})\n",
      "Stack trace:\n",
      "  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)\n",
      "  1: tvm::te::InferBound(tvm::te::Schedule const&)\n",
      "  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const\n",
      "  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const\n",
      "  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_forEiiRKSt8functionIFviEEiSD_IFSt6vectorISI_IiSaIiEESaISK_EEiiiiEEEUlRKSK_SH_E_SJ_FvSQ_SH_EE6_M_runESQ_SH_EUlvE_vEEE9_M_in\n",
      "  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)\n",
      "  6: __pthread_once_slow\n",
      "        at /build/glibc-uZu3wS/glibc-2.27/nptl/pthread_once.c:116\n",
      "  7: void std::call_once<void (std::__future_base::_State_baseV2::*)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*>(std::once_flag&, void (std::__future_base::_State_baseV2::*&&)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*&&, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*&&, bool*&&)\n",
      "  8: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()\n",
      "  9: 0x00007f77eed5c6de\n",
      "  10: start_thread\n",
      "        at /build/glibc-uZu3wS/glibc-2.27/nptl/pthread_create.c:463\n",
      "  11: __clone\n",
      "  12: 0xffffffffffffffff\n",
      "\n",
      "\n",
      "\n",
      "[11:41:06] ../src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:\n",
      "Placeholder: placeholder\n",
      "tensor.repl auto_unroll: 16\n",
      "parallel ax0@ax1@ (0,40)\n",
      "  for ax2 (0,56)\n",
      "    tensor.rf auto_unroll: 64\n",
      "    for ax0 (None)\n",
      "      for ax1 (None)\n",
      "        for ax2 (None)\n",
      "          for ax3 (None)\n",
      "            for ax4 (None)\n",
      "              for rv0_rv1_fused_o (None)\n",
      "                for rv0_rv1_fused_i (None)\n",
      "                  tensor.rf = ...\n",
      "    for ax3 (0,56)\n",
      "      for ax0 (None)\n",
      "        for ax1 (None)\n",
      "          for ax2 (None)\n",
      "            for ax3 (None)\n",
      "              vectorize ax4 (None)\n",
      "                pad_temp = ...\n",
      "      for ax4 (0,16)\n",
      "        for rv0_rv1_fused_o_v (0,3)\n",
      "          tensor.repl = ...\n",
      "\n",
      "with: [11:41:06] ../src/te/schedule/bound.cc:175: \n",
      "---------------------------------------------------------------\n",
      "An error occurred during the execution of TVM.\n",
      "For more information, please see: https://tvm.apache.org/docs/errors.html\n",
      "---------------------------------------------------------------\n",
      "  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(pad_temp, body=[tir.if_then_else(((((ax2 >= 1) && (ax2 < 113)) && (ax3 >= 1)) && (ax3 < 113)), placeholder[ax0, ax1, (ax2 - 1), (ax3 - 1), ax4], -3.40282e+38f)], axis=[iter_var(ax0, range(min=0, ext=10)), iter_var(ax1, range(min=0, ext=4)), iter_var(ax2, range(min=0, ext=114)), iter_var(ax3, range(min=0, ext=114)), iter_var(ax4, range(min=0, ext=16))], reduce_axis=[], tag=elemwise, attrs={}) along the loop nest specified by compute_at of consumer compute(tensor.rf, body=[reduce(combiner=comm_reducer(result=[max(x, y)], lhs=[x], rhs=[y], identity_element=[-3.40282e+38f]), source=[pad_temp[ax0, ax1, ((ax2*2) + floordiv((rv0.rv1.fused.inner + (rv0.rv1.fused.outer*3)), 3)), ((ax3*2) + floormod((rv0.rv1.fused.inner + (rv0.rv1.fused.outer*3)), 3)), ax4]], init=[], axis=[iter_var(rv0.rv1.fused.inner, range(min=0, ext=3))], where=(bool)1, value_index=0)], axis=[iter_var(ax0, range(min=0, ext=10)), iter_var(ax1, range(min=0, ext=4)), iter_var(ax2, range(min=0, ext=56)), iter_var(ax3, range(min=0, ext=56)), iter_var(ax4, range(min=0, ext=16)), iter_var(rv0.rv1.fused.outer, range(min=0, ext=3))], reduce_axis=[iter_var(rv0.rv1.fused.inner, range(min=0, ext=3))], tag=, attrs={})\n",
      "Stack trace:\n",
      "  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)\n",
      "  1: tvm::te::InferBound(tvm::te::Schedule const&)\n",
      "  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const\n",
      "  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const\n",
      "  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_forEiiRKSt8functionIFviEEiSD_IFSt6vectorISI_IiSaIiEESaISK_EEiiiiEEEUlRKSK_SH_E_SJ_FvSQ_SH_EE6_M_runESQ_SH_EUlvE_vEEE9_M_in\n",
      "  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)\n",
      "  6: __pthread_once_slow\n",
      "        at /build/glibc-uZu3wS/glibc-2.27/nptl/pthread_once.c:116\n",
      "  7: void std::call_once<void (std::__future_base::_State_baseV2::*)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*>(std::once_flag&, void (std::__future_base::_State_baseV2::*&&)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*&&, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*&&, bool*&&)\n",
      "  8: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()\n",
      "  9: 0x00007f77eed5c6de\n",
      "  10: start_thread\n",
      "        at /build/glibc-uZu3wS/glibc-2.27/nptl/pthread_create.c:463\n",
      "  11: __clone\n",
      "  12: 0xffffffffffffffff\n",
      "\n",
      "\n",
      "\n",
      "[11:41:06] ../src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:\n",
      "Placeholder: placeholder\n",
      "tensor.repl auto_unroll: 512\n",
      "parallel ax0@ax1@ (None)\n",
      "  tensor.rf auto_unroll: 512\n",
      "  for ax0 (None)\n",
      "    for ax1 (None)\n",
      "      for ax2 (None)\n",
      "        for ax3 (None)\n",
      "          for ax4 (None)\n",
      "            for rv0_rv1_fused_o (None)\n",
      "              vectorize rv0_rv1_fused_i (None)\n",
      "                tensor.rf = ...\n",
      "  for ax2 (None)\n",
      "    for ax0 (None)\n",
      "      for ax1 (None)\n",
      "        for ax2 (None)\n",
      "          for ax3 (None)\n",
      "            vectorize ax4 (None)\n",
      "              pad_temp = ...\n",
      "    for ax3 (None)\n",
      "      for ax4 (None)\n",
      "        for rv0_rv1_fused_i_v (None)\n",
      "          tensor.repl = ...\n",
      "\n",
      "with: [11:41:06] ../src/te/schedule/bound.cc:175: \n",
      "---------------------------------------------------------------\n",
      "An error occurred during the execution of TVM.\n",
      "For more information, please see: https://tvm.apache.org/docs/errors.html\n",
      "---------------------------------------------------------------\n",
      "  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(pad_temp, body=[tir.if_then_else(((((ax2 >= 1) && (ax2 < 113)) && (ax3 >= 1)) && (ax3 < 113)), placeholder[ax0, ax1, (ax2 - 1), (ax3 - 1), ax4], -3.40282e+38f)], axis=[iter_var(ax0, range(min=0, ext=10)), iter_var(ax1, range(min=0, ext=4)), iter_var(ax2, range(min=0, ext=114)), iter_var(ax3, range(min=0, ext=114)), iter_var(ax4, range(min=0, ext=16))], reduce_axis=[], tag=elemwise, attrs={}) along the loop nest specified by compute_at of consumer compute(tensor.rf, body=[reduce(combiner=comm_reducer(result=[max(x, y)], lhs=[x], rhs=[y], identity_element=[-3.40282e+38f]), source=[pad_temp[ax0, ax1, ((ax2*2) + floordiv((rv0.rv1.fused.inner + (rv0.rv1.fused.outer*3)), 3)), ((ax3*2) + floormod((rv0.rv1.fused.inner + (rv0.rv1.fused.outer*3)), 3)), ax4]], init=[], axis=[iter_var(rv0.rv1.fused.outer, range(min=0, ext=3))], where=(bool)1, value_index=0)], axis=[iter_var(ax0, range(min=0, ext=10)), iter_var(ax1, range(min=0, ext=4)), iter_var(ax2, range(min=0, ext=56)), iter_var(ax3, range(min=0, ext=56)), iter_var(ax4, range(min=0, ext=16)), iter_var(rv0.rv1.fused.inner, range(min=0, ext=3))], reduce_axis=[iter_var(rv0.rv1.fused.outer, range(min=0, ext=3))], tag=, attrs={})\n",
      "Stack trace:\n",
      "  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)\n",
      "  1: tvm::te::InferBound(tvm::te::Schedule const&)\n",
      "  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const\n",
      "  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const\n",
      "  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_forEiiRKSt8functionIFviEEiSD_IFSt6vectorISI_IiSaIiEESaISK_EEiiiiEEEUlRKSK_SH_E_SJ_FvSQ_SH_EE6_M_runESQ_SH_EUlvE_vEEE9_M_in\n",
      "  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)\n",
      "  6: __pthread_once_slow\n",
      "        at /build/glibc-uZu3wS/glibc-2.27/nptl/pthread_once.c:116\n",
      "  7: void std::call_once<void (std::__future_base::_State_baseV2::*)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*>(std::once_flag&, void (std::__future_base::_State_baseV2::*&&)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*&&, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*&&, bool*&&)\n",
      "  8: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()\n",
      "  9: 0x00007f77eed5c6de\n",
      "  10: start_thread\n",
      "        at /build/glibc-uZu3wS/glibc-2.27/nptl/pthread_create.c:463\n",
      "  11: __clone\n",
      "  12: 0xffffffffffffffff\n",
      "\n",
      "\n",
      "\n",
      "[11:41:09] ../src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:\n",
      "Placeholder: placeholder\n",
      "parallel ax0@ax1@ (None)\n",
      "  for ax2 (None)\n",
      "    tensor.rf auto_unroll: 64\n",
      "    for ax0 (None)\n",
      "      for ax1 (None)\n",
      "        for ax2 (None)\n",
      "          for ax3 (None)\n",
      "            for ax4 (None)\n",
      "              for rv0_rv1_fused_o (None)\n",
      "                for rv0_rv1_fused_i (None)\n",
      "                  tensor.rf = ...\n",
      "    for ax3 (None)\n",
      "      for ax0 (None)\n",
      "        for ax1 (None)\n",
      "          for ax2 (None)\n",
      "            for ax3 (None)\n",
      "              vectorize ax4 (None)\n",
      "                pad_temp = ...\n",
      "      for ax4 (None)\n",
      "        for rv0_rv1_fused_o_v (None)\n",
      "          tensor.repl = ...\n",
      "\n",
      "with: [11:41:09] ../src/te/schedule/bound.cc:175: \n",
      "---------------------------------------------------------------\n",
      "An error occurred during the execution of TVM.\n",
      "For more information, please see: https://tvm.apache.org/docs/errors.html\n",
      "---------------------------------------------------------------\n",
      "  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(pad_temp, body=[tir.if_then_else(((((ax2 >= 1) && (ax2 < 113)) && (ax3 >= 1)) && (ax3 < 113)), placeholder[ax0, ax1, (ax2 - 1), (ax3 - 1), ax4], -3.40282e+38f)], axis=[iter_var(ax0, range(min=0, ext=10)), iter_var(ax1, range(min=0, ext=4)), iter_var(ax2, range(min=0, ext=114)), iter_var(ax3, range(min=0, ext=114)), iter_var(ax4, range(min=0, ext=16))], reduce_axis=[], tag=elemwise, attrs={}) along the loop nest specified by compute_at of consumer compute(tensor.rf, body=[reduce(combiner=comm_reducer(result=[max(x, y)], lhs=[x], rhs=[y], identity_element=[-3.40282e+38f]), source=[pad_temp[ax0, ax1, ((ax2*2) + floordiv((rv0.rv1.fused.inner + (rv0.rv1.fused.outer*3)), 3)), ((ax3*2) + floormod((rv0.rv1.fused.inner + (rv0.rv1.fused.outer*3)), 3)), ax4]], init=[], axis=[iter_var(rv0.rv1.fused.inner, range(min=0, ext=3))], where=(bool)1, value_index=0)], axis=[iter_var(ax0, range(min=0, ext=10)), iter_var(ax1, range(min=0, ext=4)), iter_var(ax2, range(min=0, ext=56)), iter_var(ax3, range(min=0, ext=56)), iter_var(ax4, range(min=0, ext=16)), iter_var(rv0.rv1.fused.outer, range(min=0, ext=3))], reduce_axis=[iter_var(rv0.rv1.fused.inner, range(min=0, ext=3))], tag=, attrs={})\n",
      "Stack trace:\n",
      "  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)\n",
      "  1: tvm::te::InferBound(tvm::te::Schedule const&)\n",
      "  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const\n",
      "  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const\n",
      "  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_forEiiRKSt8functionIFviEEiSD_IFSt6vectorISI_IiSaIiEESaISK_EEiiiiEEEUlRKSK_SH_E_SJ_FvSQ_SH_EE6_M_runESQ_SH_EUlvE_vEEE9_M_in\n",
      "  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)\n",
      "  6: __pthread_once_slow\n",
      "        at /build/glibc-uZu3wS/glibc-2.27/nptl/pthread_once.c:116\n",
      "  7: void std::call_once<void (std::__future_base::_State_baseV2::*)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*>(std::once_flag&, void (std::__future_base::_State_baseV2::*&&)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*&&, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*&&, bool*&&)\n",
      "  8: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()\n",
      "  9: 0x00007f77eed5c6de\n",
      "  10: start_thread\n",
      "        at /build/glibc-uZu3wS/glibc-2.27/nptl/pthread_create.c:463\n",
      "  11: __clone\n",
      "  12: 0xffffffffffffffff\n",
      "\n",
      "\n",
      "\n",
      "[11:41:09] ../src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:\n",
      "Placeholder: placeholder\n",
      "tensor.repl auto_unroll: 512\n",
      "parallel ax0@ax1@ (0,40)\n",
      "  for ax0 (None)\n",
      "    for ax1 (None)\n",
      "      for ax2 (None)\n",
      "        for ax3 (None)\n",
      "          for ax4 (None)\n",
      "            for rv0_rv1_fused_o (None)\n",
      "              for rv0_rv1_fused_i (None)\n",
      "                tensor.rf = ...\n",
      "  for ax2 (0,56)\n",
      "    for ax3 (0,56)\n",
      "      for ax0 (None)\n",
      "        for ax1 (None)\n",
      "          for ax2 (None)\n",
      "            for ax3 (None)\n",
      "              vectorize ax4 (None)\n",
      "                pad_temp = ...\n",
      "      for ax4 (0,16)\n",
      "        for rv0_rv1_fused_o_v (0,3)\n",
      "          tensor.repl = ...\n",
      "\n",
      "with: [11:41:09] ../src/te/schedule/bound.cc:175: \n",
      "---------------------------------------------------------------\n",
      "An error occurred during the execution of TVM.\n",
      "For more information, please see: https://tvm.apache.org/docs/errors.html\n",
      "---------------------------------------------------------------\n",
      "  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(pad_temp, body=[tir.if_then_else(((((ax2 >= 1) && (ax2 < 113)) && (ax3 >= 1)) && (ax3 < 113)), placeholder[ax0, ax1, (ax2 - 1), (ax3 - 1), ax4], -3.40282e+38f)], axis=[iter_var(ax0, range(min=0, ext=10)), iter_var(ax1, range(min=0, ext=4)), iter_var(ax2, range(min=0, ext=114)), iter_var(ax3, range(min=0, ext=114)), iter_var(ax4, range(min=0, ext=16))], reduce_axis=[], tag=elemwise, attrs={}) along the loop nest specified by compute_at of consumer compute(tensor.rf, body=[reduce(combiner=comm_reducer(result=[max(x, y)], lhs=[x], rhs=[y], identity_element=[-3.40282e+38f]), source=[pad_temp[ax0, ax1, ((ax2*2) + floordiv((rv0.rv1.fused.inner + (rv0.rv1.fused.outer*3)), 3)), ((ax3*2) + floormod((rv0.rv1.fused.inner + (rv0.rv1.fused.outer*3)), 3)), ax4]], init=[], axis=[iter_var(rv0.rv1.fused.inner, range(min=0, ext=3))], where=(bool)1, value_index=0)], axis=[iter_var(ax0, range(min=0, ext=10)), iter_var(ax1, range(min=0, ext=4)), iter_var(ax2, range(min=0, ext=56)), iter_var(ax3, range(min=0, ext=56)), iter_var(ax4, range(min=0, ext=16)), iter_var(rv0.rv1.fused.outer, range(min=0, ext=3))], reduce_axis=[iter_var(rv0.rv1.fused.inner, range(min=0, ext=3))], tag=, attrs={})\n",
      "Stack trace:\n",
      "  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)\n",
      "  1: tvm::te::InferBound(tvm::te::Schedule const&)\n",
      "  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const\n",
      "  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const\n",
      "  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_forEiiRKSt8functionIFviEEiSD_IFSt6vectorISI_IiSaIiEESaISK_EEiiiiEEEUlRKSK_SH_E_SJ_FvSQ_SH_EE6_M_runESQ_SH_EUlvE_vEEE9_M_in\n",
      "  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)\n",
      "  6: __pthread_once_slow\n",
      "        at /build/glibc-uZu3wS/glibc-2.27/nptl/pthread_once.c:116\n",
      "  7: void std::call_once<void (std::__future_base::_State_baseV2::*)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*>(std::once_flag&, void (std::__future_base::_State_baseV2::*&&)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*&&, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*&&, bool*&&)\n",
      "  8: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()\n",
      "  9: 0x00007f77eed5c6de\n",
      "  10: start_thread\n",
      "        at /build/glibc-uZu3wS/glibc-2.27/nptl/pthread_create.c:463\n",
      "  11: __clone\n",
      "  12: 0xffffffffffffffff\n",
      "\n",
      "\n",
      "\n",
      "/home/mac_yuan/.local/lib/python3.9/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31-05-2022-11:55:19][AnsorEngine] Tuning Success, configuration file saved at outputs/tuninglog_network_name=myresnet--target=llvm -mcpu cascadelake_finished.json\n",
      "[31-05-2022-11:55:19][AnsorEngine] Compile from outputs/tuninglog_network_name=myresnet--target=llvm -mcpu cascadelake_finished.json\n",
      "[31-05-2022-11:55:50][AnsorEngine] Compile success.\n"
     ]
    }
   ],
   "source": [
    "optimum = Optimum(\"myresnet\")\n",
    "optimum.run(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load optimized model and make a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31-05-2022-11:55:58][Optimum] Load module from ./outputs/optimized_model/\n",
      "[31-05-2022-11:55:58][Optimum] Compile success.\n",
      "{'output_0': array([[-30.162037  ,  35.15393   , -11.729017  , ..., -39.02967   ,\n",
      "         19.354353  ,   4.572182  ],\n",
      "       [ -1.2263709 ,   0.95507854,   1.9208261 , ...,  -0.93116546,\n",
      "         -0.04104741,   2.6640966 ],\n",
      "       [ -1.2506486 ,   0.9526835 ,   1.652109  , ...,  -0.86913544,\n",
      "          0.15493488,   2.5679255 ],\n",
      "       ...,\n",
      "       [ -1.5307267 ,   1.0663735 ,   1.5047631 , ...,  -1.2226639 ,\n",
      "         -0.07349799,   3.0583618 ],\n",
      "       [ -0.5771091 ,   1.3409051 ,   1.6112486 , ...,  -0.77533424,\n",
      "         -0.20042115,   2.7553709 ],\n",
      "       [ -1.5438777 ,   1.453372  ,   1.6685232 , ...,  -1.2125641 ,\n",
      "          0.0426856 ,   2.9649062 ]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "inputs_dict = {}\n",
    "predict_model = optimum.load_model(\"./outputs/optimized_model/\", \"llvm -mcpu cascadelake\")\n",
    "result = predict_model.predict(inputs_dict)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
